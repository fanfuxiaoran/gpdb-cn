<!-- --- title: gpdb5官方安装手册-->

[[_TOC_|level = 3]]

# 0. 概述

Master:

    1. 保存系统的catalog, 但不保存用户数据, 因此对磁盘的大小和性能要求不高
    2. 因为要处理大连的用户连接以及数据加载, 对CPU的要求比较高
    3. 如果master上运行其他的工具,比如ETL或者报表工具, 需要对磁盘,CPU,内存资源重新考量

Standby:

    1. master服务器的热备
    2. 可以将standby部署在segments中的一台主机上, 也可以单独部署.
    3. master挂掉后, master停止log复制进程, 这时管理员可以激活standby服务器,升级为master角色
       standby激活后, 复制的log用于重建master的状态, 此时master处于最后的成功提交日志的时间点.
    4. master和Standby始终保持系统catalog的同步

Segments:

    1. 保存用户数据
    2. 用户定义表的结构和索引也会分布在不同的segments上
    3. 每个segment只会保存一部分数据
    4. Segment实例是运行在segment主机上的数据库进程, 每台segment主机可以运行多个segment
    5. 用户不能直接与segment主机交互, 必须要通过master.
    6. 每个segmnet主机上,运行多少个segment实例, 取决于该主机的CPU的核心数
        
        如果是2个双核心CPU: 可以运行2或4个segment实例
        如果是3个4核心CPU:  可以运行3或6或12个segment实例
        具体还要经过性能测试才可以最终确定
        
Mirror Segments:

    1. Mirror segments是可选的.
    2. Mirror segments是primary segments的备份, 在主数据失效的情况, 可以查询mirror数据节点.
    3. Mirror segment与Primary segment一定不会在同一台机器上.

        Group Mirroring:
    
            1. 这是mirror数据分布的默认配置
            2. 将某台segment主机上的所有primary segment的数据, 保存在另外一台服务器.

        Spread Mirroring:
    
            1. 将每台segment主机上的每个primary segment实例的数据,分散在剩余的其他的segment服务器群中.

    4. 每个segment主机有多个网卡, primary和mirror segments的流量均匀的分布在不同的网卡上.

Segment容灾和恢复

    1. 只有启用了Mirror segments, 才具有容灾和恢复的特性
    2. 如果primary segment失效, GPDB会自动切换到mirror segment.
    3. GPDB可以允许一台segment主机down掉, 只要保证数据时全量的, gpdb就是可用的.
    4. 如果master无法连接segment主机, GPDB会在catalog中将该segment标记为invalid

        1. 需要管理员介入, 恢复down掉的segment主机
        2. 恢复的思路: 管理员需要将该segment不可操作时间段的数据恢复到该segment上, 并且启动segment
        3. 管理员可以在GPDB运行期间恢复segment主机.

    5. 如果GPDB没有启动Mirror功能, 如果出现Segment主机down掉的情况, GPDB会自动关闭, 管理员修复后, 才能继续工作.

硬件考量:

    1. 考量的重点在segment主机的配置上, 因为这里是gpdb的工作负载最大的地方.
    2. GPDB的整体性能, 是由性能最低的Segment主机的配置决定的.(符合木桶原理)
    3. Segment主机的配置最好是一致的, 并且调优到最佳性能状态.(!!硬件一致, 配置一致!!)
    4. Segment主机应该是专用的, 不要在上面运行其他应用或服务
    5. Segment主机的CPU核心数, 决定了可以运行多少个primary segment实例

磁盘布局:

    1. 每个CPU映射到一个逻辑磁盘上
    2. 逻辑磁盘上可以存放primary数据(也可以存放mirror数据)

网络:

    1. 推荐使用10 Gigabit Ethernet switching fabric.
    2. GPDB默认使用UDP进行数据交换, GPDB自动控制数据包的检验, 因此可靠性可以与TCP相当, 但性能会更好.
        
        可以通过`gp_interconnect_type`,指定连接的类型.
        
    3. 高可用方案, 可以考虑在master与segments主机之间, 部署双-10Gigabit Ethernet switches
    4. 每台segment主机一般会有多个网卡, master一般还会有一块external网卡, 用于用户连接gpdb.
    5. 在具有多网卡的segment主机上, 可以将流量分布在不同的网卡上.

        思路: 通过将不同的segment实例, 绑定到不同的网卡上实现.
        
        实现: 给同一台服务器, 起不同的主机名, 不同的主机名绑定到不同的地址上.
        
            例如: 一台具有4块网卡的segment主机, 每个网卡有不同的IP地址, 可以将这些信息写到/etc/hosts中
            
            /etc/hosts应该包括了: 
            
                master, standby, segments, ETL等主机的地址
                如果主机具有多个网卡, 应该也具有这些信息.
                
    6. 交换机配置: 如果使用了双 10G交换机, 换分子网要均衡

        例如: 如果有2台交换机, segment主机有4块网卡, 则:
        
            NIC1, NIC2: 连接交换机1
            NIC3, NIC4: 连接交换机2
            
            假定,master的NIC1连接到了交换机1上, 那么standby应该连接到交换机2上.
            
ETL主机:

    1. GPDB支持快速,并行从外部表加载数据: 外部表 + gpfdist实现
    2. ETL服务器上一般会部署gpfdist, 但不会部署Greenplum数据库实例.
    3. gpfdist的好处: 可以确保所有的segments主机,最大带宽的读取外部表数据.

        读取分割文件: 350MB/s
        读取CSV文件: 200MB/s
    4. 如果ETL主机具有多网卡, 应该在/etc/hosts中,为每个地址添加一条记录, 在CREATE EXTERNAL TABLE中的LOCATION中声明.

性能监控:

    含糊不清, 但是需要在每台主机上安装agent, 默认15秒发送一次数据, 支持查询结构和web界面
    
# 1. 容量评估

## 1.1 计算可用磁盘容量:usable_disk_space

    每台segment服务器的可用磁盘容量 * segment服务器数量 = 整个Greenplum集群的可用磁盘容量

单台segment服务器的物理磁盘容量: raw_capacity

    raw_capacity = disk_size * number_of_disks
    
预估格式化后, 单台segment服务器的磁盘空间: formatted_disk_space, 还要考虑磁盘的RAID情况:

    假如这里的磁盘RAID为: RAID 1 + 0

    formatted_disk_space = (raw_capacity * 0.9) / 2
    
考虑到性能问题, 磁盘不能被占满, 因此有效存储空间, 应该为formatted_disk_space的70%以下.

    usable_disk_space = formatted_disk_space * 0.7
    
如果假定用户的数据量为:U

    1. 如果启用了Mirror功能, 理论上: usable_disk_space必须大于: 2 * U
    2. 由于GPDB运行时, 还需要一些磁盘空间作为工作区, 大约为: U / 3

    启用镜像:
    
        usable_disk_space = (2 * U) + U/3
        
    不启用镜像:
    
        usable_disk_space = U + U/3
        
## 1.2 计算用户数据大小

> 假定原始数据量为U, 则导入数据库后, 占用的磁盘空间大约为: 1.4 * U

这些多出来的数据量包括:

    1. Page Overhead:

        GPDB中数据时按page存储的, 每个page 32KB, Page Overhead为20字节
        
    2. Row Overhead:

        在普通的heap存储表中, 每行数据有24字节的Row Overhead
        append-optimized存储表中, 每行数据有4字节的Row Overhead
        
    3. Attribute Overhead:
    
        与选择的数据类型有关

    4. Indexes:

        索引默认分布在不同的segment上, GPDB的默认索引为B-tree
        
        因为索引大小与数据规模有关, 因此不可能提前准确预估, 可以参考如下公式:
        
        B-tree:
        
            unique_values * (data_type_size + 24 bytes)
            
        Bitmap:
        
            (unique_values * number_of_rows * 1 bit * compression_ratio / 8) + (unique_values * 32)
            
## 1.3 计算metadta和log的磁盘空间

System Metadata:

    每个segment实例或master实例, 大约需要20MB.
    
Write Ahead Log:

    WAL的文件数 = 2 * checkpoint_segments + 1
    WAL的磁盘空间= 64MB * WAL的文件数
    
    checkpoint_segments的默认值一般为:8, WAL的磁盘空间=  64MB * (2 * 8 +1) = 1088MB
    
Greenplum Database Log Files:

    需要自己想办法搞定日志轮转, 转存或容量限制的问题
    
Command Center Data:

    master或segments服务器上安装了agent, 可以收集系统性能数据, 这些数据也会保存到gpdb数据库中(gpperfmon),
    因此需要考虑这部分容量, 另外, 历史数据不会自己清理, 需要管理员要定期处理.
    



    
    

    

    
        
